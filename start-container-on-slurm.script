#!/bin/bash

#SBATCH --partition=batch
#SBATCH --ntasks-per-node=1
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=16
#SBATCH --mem-per-cpu=3072
##SBATCH --time=60:00
#SBATCH --output=outfile-%J
#SBATCH --gpus=8

set -eux

## User defined variables for running BBSQL
export MOUNT=/data
export CONCURRENTGPU='1'
export QUERY='Q8'
export DRIVER_MEMORY='10240'
export PARTITIONBYTES='512M'
export PARTITIONS='128'
export BROADCASTTHRESHOLD='512M'
export INPUT_PATH="file:///opt/parquet"
export OUTPUT_PATH="file:///opt/results"
export WAREHOUSE_PATH="file:///tmp"
export S3_ENDPOINT=""
export S3A_CREDS_USR=""
export S3A_CREDS_PSW=""
export PARQUET_URL="https://cloud.swiftstack.com/v1/AUTH_eric/downloads/1gb-parquet.tar"

## Only used to build container images
##wget -c http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz -O - | sudo tar --strip-components=1 --one-top-level=/data/spark -xz
##JARS_URL="https://cloud.swiftstack.com/v1/AUTH_eric/downloads/rapids-4-spark-integration-tests_2.12-0.1-SNAPSHOT.jar"
##BBSQL_URL="https://cloud.swiftstack.com/v1/AUTH_eric/downloads/bbsql_apps-0.2.2-SNAPSHOT.jar"

export MASTER="spark://`hostname`:7077"
export GPU_PER_NODE=$(( ${SLURM_GPUS} / ${SLURM_JOB_NUM_NODES} ))
export TOTAL_CORES=$(( ${SLURM_CPUS_PER_TASK} * ${SLURM_NTASKS} ))
export NUM_EXECUTORS=$SLURM_GPUS
export NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
export SPARK_WORKER_CORES=`nproc`
export SPARK_WORKER_MEMORY=$(( $SPARK_WORKER_CORES * $SLURM_MEM_PER_CPU ))M
export RESOURCE_GPU_AMT=$(echo $(( 1000 * ${NUM_EXECUTORS} / ${TOTAL_CORES} )) |sed 's/...$/.&/')
export WORKER_OPTS="-Dspark.worker.resource.gpu.amount=$GPU_PER_NODE -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"

sudo mkdir -p ${MOUNT}/conf
sudo chown -R $(id -u):$(id -g) ${MOUNT}/conf
sudo cp wait-worker.sh ${MOUNT}/conf/wait-worker.sh
sudo chmod +x ${MOUNT}/conf/wait-worker.sh

srun -n 1 -N 1 --gpus=0 -w `hostname` docker kill master &
sleep 2
srun --ntasks="${SLURM_JOB_NUM_NODES}" docker kill worker &
sleep 2
srun -n 1 -N 1 --gpus=0 -w `hostname` docker rm master &
sleep 2
srun --ntasks="${SLURM_JOB_NUM_NODES}" docker rm worker &
sleep 2
srun --ntasks="${SLURM_JOB_NUM_NODES}" bash -c "echo -n 'Clearing cache on ' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3"

scontrol show hostname $SLURM_JOB_NODELIST > ${MOUNT}/conf/slaves

env=${MOUNT}/conf/spark-env.sh
echo "export CONCURRENTGPU=$CONCURRENTGPU" >> $env
echo "export MASTER=$MASTER" >> $env
echo "export SPARK_WORKER_CORES=$SPARK_WORKER_CORES" >> $env
echo "export SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY" >> $env
echo "export GPU_PER_NODE=$GPU_PER_NODE" >> $env
echo "export TOTAL_CORES=$TOTAL_CORES" >> $env
echo "export NUM_EXECUTORS=$NUM_EXECUTORS" >> $env
echo "export NUM_EXECUTOR_CORES=$NUM_EXECUTOR_CORES" >> $env
echo "export RESOURCE_GPU_AMT=$RESOURCE_GPU_AMT" >> $env
echo "export WORKER_OPTS='$WORKER_OPTS'" >> $env
sudo chmod +x ${MOUNT}/conf/spark-env.sh

conf=${MOUNT}/conf/spark-defaults.conf
echo "spark.default.parallelism" $(( $SLURM_CPUS_PER_TASK * $SLURM_NTASKS )) > $conf
echo "spark.submit.deployMode" client >> $conf
echo "spark.master" spark://`hostname`:7077 >> $conf
echo "spark.executor.cores" $NUM_EXECUTOR_CORES >> $conf
echo "spark.executor.memory" $((( $SLURM_CPUS_PER_TASK * $SLURM_MEM_PER_CPU / $GPU_PER_NODE )))M >> $conf

mkdir -p ${MOUNT}/results
sudo chown -R $(id -u):$(id -g) ${MOUNT}/results
mkdir -p ${MOUNT}/history
sudo chown -R $(id -u):$(id -g) ${MOUNT}/history

if [ ! -d "${MOUNT}/parquet/customer" ]
then
    wget -c ${PARQUET_URL} -O - | sudo tar --strip-components=1 --one-top-level=${MOUNT}/parquet -x

else
    echo "${MOUNT}/parquet/customer exists"
fi

##srun -n 1 -N 1 --gpus=0 -w `hostname` docker rmi gcr.io/data-science-enterprise/spark-master-rapids-cuda:3.0.1

srun -n 1 -N 1 --gpus=0 -w `hostname` docker run -dit \
-v ${MOUNT}/conf/spark-env.sh:/opt/spark/conf/spark-env.sh \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}/history:/opt/spark/history \
-v ${MOUNT}/parquet:/opt/parquet \
-v ${MOUNT}/results:/opt/results \
-e MASTER=$MASTER \
--network host \
--name master \
--rm \
gcr.io/data-science-enterprise/spark-master-rapids-cuda:3.0.1

##srun -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 docker rmi gcr.io/data-science-enterprise/spark-worker-rapids-cuda:3.0.1

srun -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 docker run -dit \
-e MASTER=$MASTER \
-e RESOURCE_GPU_AMT="$RESOURCE_GPU_AMT" \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY \
-e SPARK_WORKER_OPTS="$WORKER_OPTS" \
-e NUM_EXECUTORS="$NUM_EXECUTORS" \
-v ${MOUNT}/conf/spark-env.sh:/opt/spark/conf/spark-env.sh \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}/history:/opt/spark/history \
-v ${MOUNT}/parquet:/opt/parquet \
-v ${MOUNT}/results:/opt/results \
--network host \
--name worker \
--rm \
gcr.io/data-science-enterprise/spark-worker-rapids-cuda:3.0.1

srun -n 1 -N 1 --gpus=0 -w `hostname` bash -c  echo `hostname` && $MOUNT/conf/wait-worker.sh

echo "All workers registered!"

##srun -n 1 -N 1 --gpus=0 docker rmi gcr.io/data-science-enterprise/spark-3.0.1-bbsql-0.2.2-rapids-2.12-0.1-cuda

srun -n 1 -N 1 --gpus=0 docker run -i \
-e MASTER=$MASTER \
-e RESOURCE_GPU_AMT="$RESOURCE_GPU_AMT" \
-e SPARK_WORKER_CORES=`nproc` \
-e SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY \
-e SPARK_WORKER_OPTS="$WORKER_OPTS" \
-e CONCURRENTGPU="$CONCURRENTGPU" \
-e TOTAL_CORES="$TOTAL_CORES" \
-e NUM_EXECUTORS="$NUM_EXECUTORS" \
-e NUM_EXECUTOR_CORES="$NUM_EXECUTOR_CORES" \
-e S3A_CREDS_USR="$S3A_CREDS_USR" \
-e S3A_CREDS_PSW="$S3A_CREDS_PSW" \
-e S3_ENDPOINT="$S3_ENDPOINT" \
-e WAREHOUSE_PATH="$WAREHOUSE_PATH" \
-e OUTPUT_PATH="$OUTPUT_PATH" \
-e INPUT_PATH="$INPUT_PATH" \
-e BROADCASTTHRESHOLD="$BROADCASTTHRESHOLD" \
-e PARTITIONS="$PARTITIONS" \
-e PARTITIONBYTES="$PARTITIONBYTES" \
-e DRIVER_MEMORY="$DRIVER_MEMORY" \
-e QUERY="$QUERY" \
-v ${MOUNT}/conf/spark-env.sh:/opt/spark/conf/spark-env.sh \
-v ${MOUNT}/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf \
-v ${MOUNT}/history:/opt/spark/history \
-v ${MOUNT}/parquet:/opt/parquet \
-v ${MOUNT}/results:/opt/results \
-v /tmp:/tmp \
--network host \
--name submit \
--rm \
gcr.io/data-science-enterprise/spark-3.0.1-bbsql-0.2.2-rapids-2.12-0.1-cuda:0.1

echo "test complete, check ${MOUNT}/results and ${MOUNT}/history" 

srun -pdebug -n 1 -N 1 --gpus=0 docker kill master
srun -pdebug -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 --gpus=0 docker kill worker

##sleep infinity
